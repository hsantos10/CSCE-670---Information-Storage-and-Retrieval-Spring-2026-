{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5qauRBKHyjk"
      },
      "source": [
        "# Information Retrieval: Vector Space Model & Evaluation\n",
        "\n",
        "This notebook covers key concepts in Information Retrieval including:\n",
        "- Vector Space Model\n",
        "- TF-IDF Weighting\n",
        "- Cosine Similarity\n",
        "- Evaluation Metrics (Precision, Recall, F-measure, MAP)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/hsantos10/CSCE-670---Information-Storage-and-Retrieval-Spring-2026-/blob/main/Week%206/Week_06_Recommender_Basics.ipynb"
      ],
      "metadata": {
        "id": "uB6HlKZpHzzq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VE1ty1DJHyjl"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import seaborn as sns\n",
        "\n",
        "# Set style for better visualizations\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (10, 6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRNnK3r8Hyjl"
      },
      "source": [
        "## 1. Vector Space Model (VSM)\n",
        "\n",
        "### Concept\n",
        "The Vector Space Model represents documents and queries as vectors in a high-dimensional space where:\n",
        "- Each dimension corresponds to a term in the vocabulary\n",
        "- Documents with similar content will be close in this space\n",
        "- Similarity can be computed using geometric measures (e.g., cosine similarity)\n",
        "\n",
        "### Mathematical Representation\n",
        "- Document: $\\vec{d} = (w_{1,d}, w_{2,d}, ..., w_{n,d})$\n",
        "- Query: $\\vec{q} = (w_{1,q}, w_{2,q}, ..., w_{n,q})$\n",
        "- Where $w_{i,j}$ is the weight of term $i$ in document/query $j$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xMUBEdr0Hyjl"
      },
      "outputs": [],
      "source": [
        "# Example document collection\n",
        "documents = [\n",
        "    \"The cat sat on the mat\",\n",
        "    \"The dog sat on the log\",\n",
        "    \"Cats and dogs are animals\",\n",
        "    \"The animal sat on the chair\"\n",
        "]\n",
        "\n",
        "query = \"cat and dog\"\n",
        "\n",
        "print(\"Document Collection:\")\n",
        "for i, doc in enumerate(documents, 1):\n",
        "    print(f\"Doc {i}: {doc}\")\n",
        "print(f\"\\nQuery: {query}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_49as9SzHyjl"
      },
      "source": [
        "## 2. Term Frequency (TF)\n",
        "\n",
        "### Raw Term Frequency\n",
        "$$TF(t, d) = \\text{count of term } t \\text{ in document } d$$\n",
        "\n",
        "### Normalized Term Frequency\n",
        "$$TF(t, d) = \\frac{\\text{count of term } t \\text{ in document } d}{\\text{total terms in document } d}$$\n",
        "\n",
        "### Log-scaled Term Frequency\n",
        "$$TF(t, d) = 1 + \\log(\\text{count of term } t \\text{ in document } d)$$\n",
        "(if count > 0, else 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RQYtRih5Hyjm"
      },
      "outputs": [],
      "source": [
        "def calculate_term_frequency(document):\n",
        "    \"\"\"Calculate raw term frequency for a document\"\"\"\n",
        "    terms = document.lower().split()\n",
        "    tf = Counter(terms)\n",
        "    return dict(tf)\n",
        "\n",
        "def calculate_normalized_tf(document):\n",
        "    \"\"\"Calculate normalized term frequency\"\"\"\n",
        "    terms = document.lower().split()\n",
        "    tf = Counter(terms)\n",
        "    total_terms = len(terms)\n",
        "    return {term: count/total_terms for term, count in tf.items()}\n",
        "\n",
        "def calculate_log_tf(document):\n",
        "    \"\"\"Calculate log-scaled term frequency\"\"\"\n",
        "    terms = document.lower().split()\n",
        "    tf = Counter(terms)\n",
        "    return {term: 1 + np.log10(count) if count > 0 else 0 for term, count in tf.items()}\n",
        "\n",
        "# Example with first document\n",
        "example_doc = documents[0]\n",
        "print(f\"Document: '{example_doc}'\\n\")\n",
        "print(\"Raw TF:\", calculate_term_frequency(example_doc))\n",
        "print(\"\\nNormalized TF:\", calculate_normalized_tf(example_doc))\n",
        "print(\"\\nLog TF:\", calculate_log_tf(example_doc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hM8tBxf5Hyjm"
      },
      "source": [
        "## 3. Inverse Document Frequency (IDF)\n",
        "\n",
        "### Concept\n",
        "IDF measures how informative a term is across the entire document collection.\n",
        "- Rare terms have high IDF (more informative)\n",
        "- Common terms have low IDF (less informative)\n",
        "\n",
        "### Formula\n",
        "$$IDF(t) = \\log\\frac{N}{df(t)}$$\n",
        "\n",
        "Where:\n",
        "- $N$ = total number of documents\n",
        "- $df(t)$ = number of documents containing term $t$\n",
        "\n",
        "### Alternative (smoothed) Formula\n",
        "$$IDF(t) = \\log\\frac{N + 1}{df(t) + 1} + 1$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-rp9MKL9Hyjm"
      },
      "outputs": [],
      "source": [
        "def calculate_idf(documents):\n",
        "    \"\"\"Calculate IDF for all terms in document collection\"\"\"\n",
        "    N = len(documents)\n",
        "\n",
        "    # Count document frequency for each term\n",
        "    df = {}\n",
        "    for doc in documents:\n",
        "        terms = set(doc.lower().split())\n",
        "        for term in terms:\n",
        "            df[term] = df.get(term, 0) + 1\n",
        "\n",
        "    # Calculate IDF\n",
        "    idf = {term: np.log10(N / doc_freq) for term, doc_freq in df.items()}\n",
        "    return idf, df\n",
        "\n",
        "idf_scores, doc_frequencies = calculate_idf(documents)\n",
        "\n",
        "# Create a DataFrame for better visualization\n",
        "idf_df = pd.DataFrame({\n",
        "    'Term': list(idf_scores.keys()),\n",
        "    'Document Frequency': [doc_frequencies[term] for term in idf_scores.keys()],\n",
        "    'IDF Score': list(idf_scores.values())\n",
        "}).sort_values('IDF Score', ascending=False)\n",
        "\n",
        "print(\"IDF Scores for Terms:\")\n",
        "print(idf_df.to_string(index=False))\n",
        "\n",
        "# Visualize IDF scores\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.bar(idf_df['Term'], idf_df['IDF Score'], color='steelblue')\n",
        "plt.xlabel('Terms')\n",
        "plt.ylabel('IDF Score')\n",
        "plt.title('IDF Scores: Rare vs Common Terms')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nðŸ“Œ Observation: Terms like 'the' have low IDF (common), while 'mat', 'log' have higher IDF (rare)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7cLxFHEHyjm"
      },
      "source": [
        "## 4. TF-IDF Weighting\n",
        "\n",
        "### Formula\n",
        "$$TF\\text{-}IDF(t, d) = TF(t, d) \\times IDF(t)$$\n",
        "\n",
        "### Intuition\n",
        "- Combines term importance within a document (TF) with term rarity across documents (IDF)\n",
        "- High TF-IDF means: term appears frequently in this document but rarely in others\n",
        "- Low TF-IDF means: term is common across all documents (e.g., \"the\", \"and\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tq49eNiMHyjm"
      },
      "outputs": [],
      "source": [
        "def calculate_tfidf_manual(documents):\n",
        "    \"\"\"Calculate TF-IDF manually for better understanding\"\"\"\n",
        "    # Get IDF scores\n",
        "    idf_scores, _ = calculate_idf(documents)\n",
        "\n",
        "    # Calculate TF-IDF for each document\n",
        "    tfidf_vectors = []\n",
        "    for doc in documents:\n",
        "        tf = calculate_term_frequency(doc)\n",
        "        tfidf = {}\n",
        "        for term, freq in tf.items():\n",
        "            tfidf[term] = freq * idf_scores[term]\n",
        "        tfidf_vectors.append(tfidf)\n",
        "\n",
        "    return tfidf_vectors\n",
        "\n",
        "# Calculate TF-IDF manually\n",
        "tfidf_manual = calculate_tfidf_manual(documents)\n",
        "\n",
        "print(\"TF-IDF Vectors (Manual Calculation):\\n\")\n",
        "for i, tfidf_vec in enumerate(tfidf_manual, 1):\n",
        "    print(f\"Document {i}: {documents[i-1]}\")\n",
        "    sorted_terms = sorted(tfidf_vec.items(), key=lambda x: x[1], reverse=True)\n",
        "    for term, score in sorted_terms[:5]:  # Top 5 terms\n",
        "        print(f\"  {term}: {score:.4f}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cihyxKnBHyjm"
      },
      "outputs": [],
      "source": [
        "# Using sklearn's TfidfVectorizer for comparison\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = vectorizer.fit_transform(documents)\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "# Create DataFrame for visualization\n",
        "tfidf_df = pd.DataFrame(\n",
        "    tfidf_matrix.toarray(),\n",
        "    columns=feature_names,\n",
        "    index=[f'Doc {i+1}' for i in range(len(documents))]\n",
        ")\n",
        "\n",
        "print(\"TF-IDF Matrix (using sklearn):\")\n",
        "print(tfidf_df.round(3))\n",
        "\n",
        "# Heatmap visualization\n",
        "plt.figure(figsize=(14, 6))\n",
        "sns.heatmap(tfidf_df, annot=True, fmt='.3f', cmap='YlOrRd', cbar_kws={'label': 'TF-IDF Score'})\n",
        "plt.title('TF-IDF Matrix Heatmap')\n",
        "plt.xlabel('Terms')\n",
        "plt.ylabel('Documents')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8oNSU6cUHyjm"
      },
      "source": [
        "## 5. Cosine Similarity\n",
        "\n",
        "### Formula\n",
        "$$\\text{cosine similarity}(\\vec{d}, \\vec{q}) = \\frac{\\vec{d} \\cdot \\vec{q}}{||\\vec{d}|| \\times ||\\vec{q}||} = \\frac{\\sum_{i=1}^{n} d_i \\times q_i}{\\sqrt{\\sum_{i=1}^{n} d_i^2} \\times \\sqrt{\\sum_{i=1}^{n} q_i^2}}$$\n",
        "\n",
        "### Properties\n",
        "- Range: [-1, 1] (but typically [0, 1] for text)\n",
        "- 1 = identical direction (very similar)\n",
        "- 0 = orthogonal (no similarity)\n",
        "- Independent of vector magnitude (normalized)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jKu-BMAuHyjm"
      },
      "outputs": [],
      "source": [
        "def cosine_similarity_manual(vec1, vec2):\n",
        "    \"\"\"Calculate cosine similarity between two vectors\"\"\"\n",
        "    # Get all unique terms\n",
        "    all_terms = set(vec1.keys()) | set(vec2.keys())\n",
        "\n",
        "    # Create vectors\n",
        "    v1 = np.array([vec1.get(term, 0) for term in all_terms])\n",
        "    v2 = np.array([vec2.get(term, 0) for term in all_terms])\n",
        "\n",
        "    # Calculate cosine similarity\n",
        "    dot_product = np.dot(v1, v2)\n",
        "    norm_v1 = np.linalg.norm(v1)\n",
        "    norm_v2 = np.linalg.norm(v2)\n",
        "\n",
        "    if norm_v1 == 0 or norm_v2 == 0:\n",
        "        return 0\n",
        "\n",
        "    return dot_product / (norm_v1 * norm_v2)\n",
        "\n",
        "# Calculate query TF-IDF\n",
        "query_tf = calculate_term_frequency(query)\n",
        "idf_scores, _ = calculate_idf(documents)\n",
        "query_tfidf = {term: freq * idf_scores.get(term, 0) for term, freq in query_tf.items()}\n",
        "\n",
        "print(f\"Query: '{query}'\")\n",
        "print(f\"Query TF-IDF: {query_tfidf}\\n\")\n",
        "\n",
        "# Calculate similarity with each document\n",
        "similarities = []\n",
        "for i, doc_tfidf in enumerate(tfidf_manual, 1):\n",
        "    sim = cosine_similarity_manual(query_tfidf, doc_tfidf)\n",
        "    similarities.append((i, sim))\n",
        "    print(f\"Similarity with Doc {i}: {sim:.4f}\")\n",
        "\n",
        "# Rank documents\n",
        "ranked_docs = sorted(similarities, key=lambda x: x[1], reverse=True)\n",
        "print(\"\\nðŸ“Š Ranking:\")\n",
        "for rank, (doc_id, sim) in enumerate(ranked_docs, 1):\n",
        "    print(f\"{rank}. Doc {doc_id} (score: {sim:.4f}): {documents[doc_id-1]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cYQhrg-3Hyjn"
      },
      "outputs": [],
      "source": [
        "# Visualize cosine similarity\n",
        "query_vector = vectorizer.transform([query])\n",
        "similarities_sklearn = cosine_similarity(query_vector, tfidf_matrix)[0]\n",
        "\n",
        "# Create visualization\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# Bar plot\n",
        "doc_labels = [f'Doc {i+1}' for i in range(len(documents))]\n",
        "ax1.bar(doc_labels, similarities_sklearn, color='coral')\n",
        "ax1.set_xlabel('Documents')\n",
        "ax1.set_ylabel('Cosine Similarity')\n",
        "ax1.set_title(f'Query-Document Similarity\\nQuery: \"{query}\"')\n",
        "ax1.set_ylim(0, 1)\n",
        "\n",
        "# Add values on bars\n",
        "for i, v in enumerate(similarities_sklearn):\n",
        "    ax1.text(i, v + 0.02, f'{v:.3f}', ha='center', va='bottom')\n",
        "\n",
        "# Radar plot\n",
        "angles = np.linspace(0, 2 * np.pi, len(documents), endpoint=False)\n",
        "similarities_plot = np.concatenate((similarities_sklearn, [similarities_sklearn[0]]))\n",
        "angles_plot = np.concatenate((angles, [angles[0]]))\n",
        "\n",
        "ax2 = plt.subplot(122, projection='polar')\n",
        "ax2.plot(angles_plot, similarities_plot, 'o-', linewidth=2, color='coral')\n",
        "ax2.fill(angles_plot, similarities_plot, alpha=0.25, color='coral')\n",
        "ax2.set_xticks(angles)\n",
        "ax2.set_xticklabels(doc_labels)\n",
        "ax2.set_ylim(0, 1)\n",
        "ax2.set_title('Similarity Radar Plot')\n",
        "ax2.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gutDjW7CHyjn"
      },
      "source": [
        "## 6. Evaluation Metrics\n",
        "\n",
        "### Precision and Recall\n",
        "\n",
        "**Precision**: What fraction of retrieved documents are relevant?\n",
        "$$Precision = \\frac{\\text{# Relevant Retrieved}}{\\text{# Retrieved}} = \\frac{TP}{TP + FP}$$\n",
        "\n",
        "**Recall**: What fraction of relevant documents are retrieved?\n",
        "$$Recall = \\frac{\\text{# Relevant Retrieved}}{\\text{# Relevant}} = \\frac{TP}{TP + FN}$$\n",
        "\n",
        "### F-Measure\n",
        "Harmonic mean of precision and recall:\n",
        "$$F_1 = 2 \\times \\frac{Precision \\times Recall}{Precision + Recall}$$\n",
        "\n",
        "Weighted F-measure:\n",
        "$$F_\\beta = (1 + \\beta^2) \\times \\frac{Precision \\times Recall}{\\beta^2 \\times Precision + Recall}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3JTYxYkpHyjn"
      },
      "outputs": [],
      "source": [
        "def calculate_precision_recall(retrieved, relevant):\n",
        "    \"\"\"\n",
        "    Calculate precision and recall\n",
        "\n",
        "    Args:\n",
        "        retrieved: set of retrieved document IDs\n",
        "        relevant: set of relevant document IDs\n",
        "    \"\"\"\n",
        "    retrieved_set = set(retrieved)\n",
        "    relevant_set = set(relevant)\n",
        "\n",
        "    true_positives = len(retrieved_set & relevant_set)\n",
        "\n",
        "    precision = true_positives / len(retrieved_set) if len(retrieved_set) > 0 else 0\n",
        "    recall = true_positives / len(relevant_set) if len(relevant_set) > 0 else 0\n",
        "\n",
        "    return precision, recall\n",
        "\n",
        "def calculate_f_measure(precision, recall, beta=1):\n",
        "    \"\"\"Calculate F-measure\"\"\"\n",
        "    if precision + recall == 0:\n",
        "        return 0\n",
        "    return (1 + beta**2) * (precision * recall) / (beta**2 * precision + recall)\n",
        "\n",
        "# Example: System retrieved docs [1, 2, 3, 4, 5]\n",
        "# Ground truth relevant docs: [2, 3, 6, 7]\n",
        "retrieved = [1, 2, 3, 4, 5]\n",
        "relevant = [2, 3, 6, 7]\n",
        "\n",
        "precision, recall = calculate_precision_recall(retrieved, relevant)\n",
        "f1 = calculate_f_measure(precision, recall)\n",
        "\n",
        "print(\"Evaluation Example:\")\n",
        "print(f\"Retrieved documents: {retrieved}\")\n",
        "print(f\"Relevant documents:  {relevant}\")\n",
        "print(f\"\\nTrue Positives: {set(retrieved) & set(relevant)}\")\n",
        "print(f\"False Positives: {set(retrieved) - set(relevant)}\")\n",
        "print(f\"False Negatives: {set(relevant) - set(retrieved)}\")\n",
        "print(f\"\\nPrecision: {precision:.3f} ({len(set(retrieved) & set(relevant))}/{len(retrieved)})\")\n",
        "print(f\"Recall:    {recall:.3f} ({len(set(retrieved) & set(relevant))}/{len(relevant)})\")\n",
        "print(f\"F1-Score:  {f1:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bS4qhbDaHyjn"
      },
      "outputs": [],
      "source": [
        "# Visualize Precision-Recall Tradeoff\n",
        "def plot_precision_recall_tradeoff():\n",
        "    \"\"\"Visualize how precision and recall change with different thresholds\"\"\"\n",
        "    # Simulated ranked results with relevance judgments\n",
        "    ranked_results = [\n",
        "        (1, True),   # (doc_id, is_relevant)\n",
        "        (2, False),\n",
        "        (3, True),\n",
        "        (4, True),\n",
        "        (5, False),\n",
        "        (6, True),\n",
        "        (7, False),\n",
        "        (8, False),\n",
        "        (9, True),\n",
        "        (10, False)\n",
        "    ]\n",
        "\n",
        "    total_relevant = sum(1 for _, rel in ranked_results if rel)\n",
        "\n",
        "    precisions = []\n",
        "    recalls = []\n",
        "\n",
        "    # Calculate precision and recall at each cutoff\n",
        "    for k in range(1, len(ranked_results) + 1):\n",
        "        retrieved = [doc_id for doc_id, _ in ranked_results[:k]]\n",
        "        relevant_retrieved = sum(1 for _, rel in ranked_results[:k] if rel)\n",
        "\n",
        "        precision = relevant_retrieved / k\n",
        "        recall = relevant_retrieved / total_relevant\n",
        "\n",
        "        precisions.append(precision)\n",
        "        recalls.append(recall)\n",
        "\n",
        "    # Plot\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "    # Precision and Recall vs K\n",
        "    k_values = list(range(1, len(ranked_results) + 1))\n",
        "    ax1.plot(k_values, precisions, 'o-', label='Precision', linewidth=2, markersize=8)\n",
        "    ax1.plot(k_values, recalls, 's-', label='Recall', linewidth=2, markersize=8)\n",
        "    ax1.set_xlabel('K (number of documents retrieved)')\n",
        "    ax1.set_ylabel('Score')\n",
        "    ax1.set_title('Precision and Recall vs K')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    ax1.set_ylim(0, 1.1)\n",
        "\n",
        "    # Precision-Recall Curve\n",
        "    ax2.plot(recalls, precisions, 'o-', linewidth=2, markersize=8, color='purple')\n",
        "    ax2.set_xlabel('Recall')\n",
        "    ax2.set_ylabel('Precision')\n",
        "    ax2.set_title('Precision-Recall Curve')\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "    ax2.set_xlim(0, 1.1)\n",
        "    ax2.set_ylim(0, 1.1)\n",
        "\n",
        "    # Add annotations for key points\n",
        "    for i in [0, 4, 9]:\n",
        "        ax2.annotate(f'K={i+1}', xy=(recalls[i], precisions[i]),\n",
        "                    xytext=(10, 10), textcoords='offset points',\n",
        "                    bbox=dict(boxstyle='round,pad=0.5', fc='yellow', alpha=0.5),\n",
        "                    arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0'))\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return precisions, recalls\n",
        "\n",
        "precisions, recalls = plot_precision_recall_tradeoff()\n",
        "print(\"\\nðŸ“Œ Observation: As we retrieve more documents (increase K), recall increases but precision typically decreases\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shW4TRrcHyjn"
      },
      "source": [
        "## 7. Average Precision (AP)\n",
        "\n",
        "### Formula\n",
        "$$AP = \\frac{1}{R} \\sum_{k=1}^{n} Precision(k) \\times rel(k)$$\n",
        "\n",
        "Where:\n",
        "- $R$ = total number of relevant documents\n",
        "- $n$ = number of retrieved documents\n",
        "- $rel(k)$ = 1 if document at rank $k$ is relevant, 0 otherwise\n",
        "\n",
        "### Interpretation\n",
        "- Rewards systems that rank relevant documents higher\n",
        "- Takes into account the position of relevant documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2fYqOZFMHyjn"
      },
      "outputs": [],
      "source": [
        "def calculate_average_precision(ranked_results, relevant_docs):\n",
        "    \"\"\"\n",
        "    Calculate Average Precision\n",
        "\n",
        "    Args:\n",
        "        ranked_results: list of retrieved document IDs in ranked order\n",
        "        relevant_docs: set of relevant document IDs\n",
        "    \"\"\"\n",
        "    relevant_set = set(relevant_docs)\n",
        "\n",
        "    precisions_at_relevant = []\n",
        "    num_relevant_seen = 0\n",
        "\n",
        "    for i, doc_id in enumerate(ranked_results, 1):\n",
        "        if doc_id in relevant_set:\n",
        "            num_relevant_seen += 1\n",
        "            precision_at_i = num_relevant_seen / i\n",
        "            precisions_at_relevant.append(precision_at_i)\n",
        "\n",
        "    if len(precisions_at_relevant) == 0:\n",
        "        return 0\n",
        "\n",
        "    return sum(precisions_at_relevant) / len(relevant_set)\n",
        "\n",
        "# Example\n",
        "ranked_results = [3, 7, 1, 5, 2, 8, 4, 6, 9, 10]\n",
        "relevant_docs = [1, 2, 3, 5]\n",
        "\n",
        "print(\"Average Precision Example:\")\n",
        "print(f\"Ranked results: {ranked_results}\")\n",
        "print(f\"Relevant docs:  {relevant_docs}\\n\")\n",
        "\n",
        "# Show step-by-step calculation\n",
        "print(\"Step-by-step calculation:\")\n",
        "num_relevant_seen = 0\n",
        "precisions_at_relevant = []\n",
        "\n",
        "for i, doc_id in enumerate(ranked_results, 1):\n",
        "    if doc_id in relevant_docs:\n",
        "        num_relevant_seen += 1\n",
        "        precision_at_i = num_relevant_seen / i\n",
        "        precisions_at_relevant.append(precision_at_i)\n",
        "        print(f\"Rank {i}: Doc {doc_id} is relevant. Precision@{i} = {num_relevant_seen}/{i} = {precision_at_i:.4f}\")\n",
        "\n",
        "ap = calculate_average_precision(ranked_results, relevant_docs)\n",
        "print(f\"\\nAverage Precision = ({' + '.join([f'{p:.4f}' for p in precisions_at_relevant])}) / {len(relevant_docs)}\")\n",
        "print(f\"                  = {sum(precisions_at_relevant):.4f} / {len(relevant_docs)}\")\n",
        "print(f\"                  = {ap:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2eFV6eBHyjn"
      },
      "source": [
        "## 8. Mean Average Precision (MAP)\n",
        "\n",
        "### Formula\n",
        "$$MAP = \\frac{1}{Q} \\sum_{q=1}^{Q} AP(q)$$\n",
        "\n",
        "Where:\n",
        "- $Q$ = number of queries\n",
        "- $AP(q)$ = Average Precision for query $q$\n",
        "\n",
        "### Interpretation\n",
        "- Single-number metric for overall system performance\n",
        "- Average of AP scores across all queries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FKK4WmlkHyjn"
      },
      "outputs": [],
      "source": [
        "def calculate_map(queries_results):\n",
        "    \"\"\"\n",
        "    Calculate Mean Average Precision\n",
        "\n",
        "    Args:\n",
        "        queries_results: list of tuples (ranked_results, relevant_docs) for each query\n",
        "    \"\"\"\n",
        "    aps = []\n",
        "    for ranked_results, relevant_docs in queries_results:\n",
        "        ap = calculate_average_precision(ranked_results, relevant_docs)\n",
        "        aps.append(ap)\n",
        "\n",
        "    return sum(aps) / len(aps) if aps else 0\n",
        "\n",
        "# Example with multiple queries\n",
        "queries_results = [\n",
        "    # Query 1\n",
        "    ([3, 7, 1, 5, 2], [1, 2, 3]),\n",
        "    # Query 2\n",
        "    ([1, 4, 2, 6, 3], [1, 3, 4]),\n",
        "    # Query 3\n",
        "    ([5, 2, 8, 1, 3], [1, 2, 5])\n",
        "]\n",
        "\n",
        "print(\"Mean Average Precision Example:\\n\")\n",
        "aps = []\n",
        "for i, (ranked_results, relevant_docs) in enumerate(queries_results, 1):\n",
        "    ap = calculate_average_precision(ranked_results, relevant_docs)\n",
        "    aps.append(ap)\n",
        "    print(f\"Query {i}:\")\n",
        "    print(f\"  Ranked: {ranked_results}\")\n",
        "    print(f\"  Relevant: {relevant_docs}\")\n",
        "    print(f\"  AP = {ap:.4f}\\n\")\n",
        "\n",
        "map_score = calculate_map(queries_results)\n",
        "print(f\"MAP = ({' + '.join([f'{ap:.4f}' for ap in aps])}) / {len(queries_results)}\")\n",
        "print(f\"    = {map_score:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LpyGV7khHyjo"
      },
      "outputs": [],
      "source": [
        "# Visualize AP for different queries\n",
        "query_names = [f'Query {i+1}' for i in range(len(queries_results))]\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# Bar plot of AP scores\n",
        "colors = ['skyblue', 'lightcoral', 'lightgreen']\n",
        "ax1.bar(query_names, aps, color=colors)\n",
        "ax1.axhline(y=map_score, color='red', linestyle='--', linewidth=2, label=f'MAP = {map_score:.4f}')\n",
        "ax1.set_ylabel('Average Precision')\n",
        "ax1.set_title('Average Precision per Query')\n",
        "ax1.legend()\n",
        "ax1.set_ylim(0, 1.1)\n",
        "\n",
        "# Add value labels\n",
        "for i, (name, ap) in enumerate(zip(query_names, aps)):\n",
        "    ax1.text(i, ap + 0.02, f'{ap:.3f}', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "# Box plot showing distribution\n",
        "ax2.boxplot([aps], labels=['AP Scores'])\n",
        "ax2.scatter([1]*len(aps), aps, color=colors, s=100, zorder=3)\n",
        "ax2.axhline(y=map_score, color='red', linestyle='--', linewidth=2, label=f'MAP = {map_score:.4f}')\n",
        "ax2.set_ylabel('Average Precision')\n",
        "ax2.set_title('Distribution of AP Scores')\n",
        "ax2.legend()\n",
        "ax2.set_ylim(0, 1.1)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FnHGy_wTHyjo"
      },
      "source": [
        "## 9. Comprehensive Example: End-to-End IR System"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zUMYXLNtHyjo"
      },
      "outputs": [],
      "source": [
        "class SimpleIRSystem:\n",
        "    \"\"\"A simple Information Retrieval System\"\"\"\n",
        "\n",
        "    def __init__(self, documents):\n",
        "        self.documents = documents\n",
        "        self.vectorizer = TfidfVectorizer()\n",
        "        self.tfidf_matrix = self.vectorizer.fit_transform(documents)\n",
        "\n",
        "    def search(self, query, top_k=5):\n",
        "        \"\"\"Search for documents relevant to query\"\"\"\n",
        "        query_vector = self.vectorizer.transform([query])\n",
        "        similarities = cosine_similarity(query_vector, self.tfidf_matrix)[0]\n",
        "\n",
        "        # Get top-k results\n",
        "        top_indices = np.argsort(similarities)[::-1][:top_k]\n",
        "\n",
        "        results = []\n",
        "        for idx in top_indices:\n",
        "            results.append({\n",
        "                'doc_id': idx,\n",
        "                'document': self.documents[idx],\n",
        "                'score': similarities[idx]\n",
        "            })\n",
        "\n",
        "        return results\n",
        "\n",
        "    def evaluate(self, query, relevant_doc_ids, top_k=5):\n",
        "        \"\"\"Evaluate search results\"\"\"\n",
        "        results = self.search(query, top_k)\n",
        "        retrieved_ids = [r['doc_id'] for r in results]\n",
        "\n",
        "        precision, recall = calculate_precision_recall(retrieved_ids, relevant_doc_ids)\n",
        "        f1 = calculate_f_measure(precision, recall)\n",
        "        ap = calculate_average_precision(retrieved_ids, relevant_doc_ids)\n",
        "\n",
        "        return {\n",
        "            'results': results,\n",
        "            'precision': precision,\n",
        "            'recall': recall,\n",
        "            'f1': f1,\n",
        "            'ap': ap\n",
        "        }\n",
        "\n",
        "# Create a larger document collection\n",
        "doc_collection = [\n",
        "    \"Machine learning is a subset of artificial intelligence\",\n",
        "    \"Deep learning uses neural networks with multiple layers\",\n",
        "    \"Natural language processing enables computers to understand human language\",\n",
        "    \"Computer vision allows machines to interpret visual information\",\n",
        "    \"Reinforcement learning is inspired by behavioral psychology\",\n",
        "    \"Supervised learning requires labeled training data\",\n",
        "    \"Unsupervised learning finds patterns in unlabeled data\",\n",
        "    \"Transfer learning reuses pre-trained models for new tasks\",\n",
        "    \"Generative models can create new data similar to training data\",\n",
        "    \"Neural networks are inspired by biological neurons in the brain\"\n",
        "]\n",
        "\n",
        "# Initialize IR system\n",
        "ir_system = SimpleIRSystem(doc_collection)\n",
        "\n",
        "# Test query\n",
        "test_query = \"neural networks and deep learning\"\n",
        "relevant_docs = [1, 9]  # Ground truth\n",
        "\n",
        "# Evaluate\n",
        "evaluation = ir_system.evaluate(test_query, relevant_docs, top_k=5)\n",
        "\n",
        "print(f\"Query: '{test_query}'\\n\")\n",
        "print(\"=\"*80)\n",
        "print(\"Search Results:\")\n",
        "print(\"=\"*80)\n",
        "for i, result in enumerate(evaluation['results'], 1):\n",
        "    relevance = \"âœ“ RELEVANT\" if result['doc_id'] in relevant_docs else \"âœ— Not relevant\"\n",
        "    print(f\"{i}. [Doc {result['doc_id']}] {relevance}\")\n",
        "    print(f\"   Score: {result['score']:.4f}\")\n",
        "    print(f\"   Text: {result['document']}\")\n",
        "    print()\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"Evaluation Metrics:\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Precision:  {evaluation['precision']:.4f}\")\n",
        "print(f\"Recall:     {evaluation['recall']:.4f}\")\n",
        "print(f\"F1-Score:   {evaluation['f1']:.4f}\")\n",
        "print(f\"Avg Precision: {evaluation['ap']:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WV5muXxMHyjo"
      },
      "source": [
        "## 10. Summary and Key Takeaways\n",
        "\n",
        "### Vector Space Model\n",
        "- âœ… Represents documents and queries as vectors\n",
        "- âœ… Enables similarity computation\n",
        "- âœ… Foundation for modern IR systems\n",
        "\n",
        "### TF-IDF\n",
        "- âœ… Balances term frequency (local) with inverse document frequency (global)\n",
        "- âœ… Reduces impact of common words\n",
        "- âœ… Highlights discriminative terms\n",
        "\n",
        "### Cosine Similarity\n",
        "- âœ… Measures angle between vectors\n",
        "- âœ… Normalized (independent of document length)\n",
        "- âœ… Range: 0 (dissimilar) to 1 (identical)\n",
        "\n",
        "### Evaluation Metrics\n",
        "- **Precision**: Quality of results (how many retrieved are relevant?)\n",
        "- **Recall**: Coverage of results (how many relevant are retrieved?)\n",
        "- **F-measure**: Harmonic mean of precision and recall\n",
        "- **Average Precision**: Rewards ranking relevant documents higher\n",
        "- **MAP**: Overall system performance across queries\n",
        "\n",
        "### Trade-offs\n",
        "- Precision â†” Recall: Improving one often hurts the other\n",
        "- Ranking matters: AP/MAP consider position of relevant documents\n",
        "- Context matters: Different applications prioritize different metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzRGFOMFHyjo"
      },
      "source": [
        "## 11. Practice Exercises\n",
        "\n",
        "Try these exercises to reinforce your understanding:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PtGXMAgDHyjo"
      },
      "outputs": [],
      "source": [
        "# Exercise 1: Calculate TF-IDF for a new document collection\n",
        "exercise_docs = [\n",
        "    \"Python is a popular programming language\",\n",
        "    \"Java is used for enterprise applications\",\n",
        "    \"Python is great for data science and machine learning\",\n",
        "    \"JavaScript is essential for web development\"\n",
        "]\n",
        "\n",
        "# TODO: Calculate TF-IDF and find the most important terms for each document\n",
        "# Your code here\n",
        "\n",
        "print(\"Exercise 1: Implement TF-IDF calculation for the exercise_docs collection\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KhrZratXHyjo"
      },
      "outputs": [],
      "source": [
        "# Exercise 2: Given these search results, calculate precision, recall, and AP\n",
        "retrieved = [1, 3, 5, 7, 9, 2, 4, 6, 8, 10]\n",
        "relevant = [1, 2, 5, 7]\n",
        "\n",
        "# TODO: Calculate precision, recall, F1, and Average Precision\n",
        "# Your code here\n",
        "\n",
        "print(\"Exercise 2: Calculate evaluation metrics for the given results\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xdqcugPJHyjo"
      },
      "outputs": [],
      "source": [
        "# Exercise 3: Compare two different ranking algorithms\n",
        "ranking_a = [2, 1, 5, 3, 7, 4, 6, 8]\n",
        "ranking_b = [1, 3, 2, 5, 4, 7, 6, 8]\n",
        "relevant = [1, 2, 3, 5]\n",
        "\n",
        "# TODO: Calculate AP for both rankings and determine which is better\n",
        "# Your code here\n",
        "\n",
        "print(\"Exercise 3: Compare the two ranking algorithms using AP\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fXIul_VHyjo"
      },
      "source": [
        "## References and Further Reading\n",
        "\n",
        "1. **Introduction to Information Retrieval** by Manning, Raghavan, and SchÃ¼tze\n",
        "   - Chapter 6: Scoring, term weighting, and the vector space model\n",
        "   - Chapter 8: Evaluation in information retrieval\n",
        "\n",
        "2. **Modern Information Retrieval** by Baeza-Yates and Ribeiro-Neto\n",
        "\n",
        "3. **Scikit-learn Documentation**\n",
        "   - TfidfVectorizer: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
        "\n",
        "4. **TREC Evaluation Resources**: https://trec.nist.gov/"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}