{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XwTGagv1OM3t"
      },
      "source": [
        "# Information Retrieval: From Vector Space to Learning to Rank\n",
        "\n",
        "**Topics Covered:**\n",
        "1. Vector Space Model & TF-IDF\n",
        "2. Cosine Similarity\n",
        "3. Rocchio Algorithm (Relevance Feedback)\n",
        "4. k-Nearest Neighbors\n",
        "5. Evaluation Metrics (Precision, Recall, MAP, nDCG)\n",
        "6. Learning to Rank: Pointwise, Pairwise, Listwise\n",
        "7. Key Algorithms: RankSVM, RankNet, LambdaMART\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy pandas matplotlib seaborn scikit-learn jupyter"
      ],
      "metadata": {
        "id": "5okodfUbPjcD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/hsantos10/CSCE-670---Information-Storage-and-Retrieval-Spring-2026-/blob/main/Week%205%3A%20Learning%20to%20Rank.ipynb)"
      ],
      "metadata": {
        "id": "27UV1pg9ON5G"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VBsHiBuvOM3u"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import seaborn as sns\n",
        "\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (10, 6)\n",
        "\n",
        "# Sample documents\n",
        "documents = [\n",
        "    \"The cat sat on the mat\",\n",
        "    \"The dog sat on the log\",\n",
        "    \"Cats and dogs are animals\",\n",
        "    \"The animal sat on the chair\"\n",
        "]\n",
        "query = \"cat and dog\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBJyqlDaOM3u"
      },
      "source": [
        "## 1. Vector Space Model & TF-IDF\n",
        "\n",
        "### Term Frequency (TF)\n",
        "$$TF(t, d) = \\text{count of term } t \\text{ in document } d$$\n",
        "\n",
        "### Inverse Document Frequency (IDF)\n",
        "$$IDF(t) = \\log\\frac{N}{df(t)}$$\n",
        "- $N$ = total documents\n",
        "- $df(t)$ = documents containing term $t$\n",
        "\n",
        "### TF-IDF Weight\n",
        "$$w(t,d) = TF(t,d) \\times IDF(t)$$\n",
        "\n",
        "**Intuition:** High weight = term is frequent in this doc but rare overall"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iVv6rexhOM3v"
      },
      "outputs": [],
      "source": [
        "# Create TF-IDF vectors\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = vectorizer.fit_transform(documents)\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "# Display as DataFrame\n",
        "tfidf_df = pd.DataFrame(\n",
        "    tfidf_matrix.toarray(),\n",
        "    columns=feature_names,\n",
        "    index=[f'Doc{i+1}' for i in range(len(documents))]\n",
        ")\n",
        "\n",
        "print(\"TF-IDF Matrix:\")\n",
        "print(tfidf_df.round(3))\n",
        "\n",
        "# Visualize\n",
        "plt.figure(figsize=(12, 5))\n",
        "sns.heatmap(tfidf_df, annot=True, fmt='.2f', cmap='YlOrRd')\n",
        "plt.title('TF-IDF Weights Heatmap')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kj6UL1tLOM3v"
      },
      "source": [
        "## 2. Cosine Similarity\n",
        "\n",
        "### Formula\n",
        "$$\\text{sim}(\\vec{q}, \\vec{d}) = \\frac{\\vec{q} \\cdot \\vec{d}}{||\\vec{q}|| \\cdot ||\\vec{d}||} = \\frac{\\sum q_i d_i}{\\sqrt{\\sum q_i^2} \\sqrt{\\sum d_i^2}}$$\n",
        "\n",
        "**Properties:**\n",
        "- Range: [0, 1] for text (since weights are non-negative)\n",
        "- 1 = identical, 0 = orthogonal\n",
        "- Length-normalized (long docs don't automatically score higher)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y6AYV1sKOM3v"
      },
      "outputs": [],
      "source": [
        "# Compute similarities\n",
        "query_vector = vectorizer.transform([query])\n",
        "similarities = cosine_similarity(query_vector, tfidf_matrix)[0]\n",
        "\n",
        "# Rank documents\n",
        "results_df = pd.DataFrame({\n",
        "    'Document': documents,\n",
        "    'Similarity': similarities\n",
        "}).sort_values('Similarity', ascending=False)\n",
        "\n",
        "print(f\"Query: '{query}'\\n\")\n",
        "print(\"Ranked Results:\")\n",
        "print(results_df.to_string(index=False))\n",
        "\n",
        "# Visualize\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.barh(range(len(documents)), similarities)\n",
        "plt.yticks(range(len(documents)), [f'Doc{i+1}' for i in range(len(documents))])\n",
        "plt.xlabel('Cosine Similarity')\n",
        "plt.title(f'Query-Document Similarity: \"{query}\"')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mp9zOnyIOM3v"
      },
      "source": [
        "## 3. Rocchio Algorithm (Relevance Feedback)\n",
        "\n",
        "### Formula\n",
        "$$\\vec{q}_{new} = \\alpha \\vec{q}_{old} + \\beta \\frac{1}{|D_r|} \\sum_{\\vec{d} \\in D_r} \\vec{d} - \\gamma \\frac{1}{|D_{nr}|} \\sum_{\\vec{d} \\in D_{nr}} \\vec{d}$$\n",
        "\n",
        "**Parameters:**\n",
        "- $\\alpha = 1.0$ (original query weight)\n",
        "- $\\beta = 0.75$ (relevant docs weight)\n",
        "- $\\gamma = 0.15$ (non-relevant docs weight)\n",
        "\n",
        "**Intuition:** Move query towards relevant docs, away from non-relevant"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P1lldctIOM3w"
      },
      "outputs": [],
      "source": [
        "def rocchio(q_vec, rel_vecs, nonrel_vecs, alpha=1.0, beta=0.75, gamma=0.15):\n",
        "    \"\"\"Apply Rocchio algorithm\"\"\"\n",
        "    q = np.array(q_vec)\n",
        "    centroid_r = np.mean(rel_vecs, axis=0) if len(rel_vecs) > 0 else np.zeros(q.shape)\n",
        "    centroid_nr = np.mean(nonrel_vecs, axis=0) if len(nonrel_vecs) > 0 else np.zeros(q.shape)\n",
        "    q_new = alpha * q + beta * centroid_r - gamma * centroid_nr\n",
        "    q_new[q_new < 0] = 0  # No negative weights\n",
        "    return q_new\n",
        "\n",
        "# User feedback: Doc3 relevant, Doc4 non-relevant\n",
        "relevant_ids = [2]\n",
        "nonrelevant_ids = [3]\n",
        "\n",
        "# Apply Rocchio\n",
        "q_new = rocchio(\n",
        "    query_vector.toarray(),\n",
        "    tfidf_matrix[relevant_ids].toarray(),\n",
        "    tfidf_matrix[nonrelevant_ids].toarray()\n",
        ")\n",
        "\n",
        "# Re-rank\n",
        "new_similarities = cosine_similarity(q_new.reshape(1, -1), tfidf_matrix)[0]\n",
        "\n",
        "# Compare\n",
        "comparison = pd.DataFrame({\n",
        "    'Document': [f'Doc{i+1}' for i in range(len(documents))],\n",
        "    'Before': similarities,\n",
        "    'After': new_similarities,\n",
        "    'Change': new_similarities - similarities\n",
        "}).sort_values('After', ascending=False)\n",
        "\n",
        "print(\"Rocchio Feedback Results:\")\n",
        "print(comparison.round(3).to_string(index=False))\n",
        "print(\"\\nâœ… Doc3 (marked relevant) moved up in ranking!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xb5TlBmOM3w"
      },
      "source": [
        "## 4. k-Nearest Neighbors (k-NN)\n",
        "\n",
        "**Concept:** Retrieve the $k$ most similar documents based on cosine similarity.\n",
        "\n",
        "This is what standard search does when you ask for \"top k results\"!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UqHoF702OM3w"
      },
      "outputs": [],
      "source": [
        "def knn_search(query, k=2):\n",
        "    q_vec = vectorizer.transform([query])\n",
        "    sims = cosine_similarity(q_vec, tfidf_matrix)[0]\n",
        "    top_k = np.argsort(sims)[::-1][:k]\n",
        "\n",
        "    print(f\"k-NN Search (k={k}):\")\n",
        "    for rank, idx in enumerate(top_k, 1):\n",
        "        print(f\"{rank}. Doc{idx+1} (score={sims[idx]:.3f}): {documents[idx]}\")\n",
        "\n",
        "knn_search(query, k=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWGcnam_OM3x"
      },
      "source": [
        "## 5. Evaluation Metrics\n",
        "\n",
        "### Precision & Recall\n",
        "$$P = \\frac{|\\text{Relevant} \\cap \\text{Retrieved}|}{|\\text{Retrieved}|}$$\n",
        "$$R = \\frac{|\\text{Relevant} \\cap \\text{Retrieved}|}{|\\text{Relevant}|}$$\n",
        "\n",
        "### F-Measure\n",
        "$$F_1 = 2 \\cdot \\frac{P \\cdot R}{P + R}$$\n",
        "\n",
        "### Average Precision (AP)\n",
        "$$AP = \\frac{1}{|R|} \\sum_{k=1}^{n} P(k) \\cdot rel(k)$$\n",
        "\n",
        "### Mean Average Precision (MAP)\n",
        "$$MAP = \\frac{1}{|Q|} \\sum_{q=1}^{|Q|} AP(q)$$\n",
        "\n",
        "### Normalized Discounted Cumulative Gain (nDCG)\n",
        "$$DCG@k = \\sum_{i=1}^{k} \\frac{2^{rel_i} - 1}{\\log_2(i + 1)}$$\n",
        "$$nDCG@k = \\frac{DCG@k}{IDCG@k}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-93VuzXFOM3x"
      },
      "outputs": [],
      "source": [
        "def precision_recall(retrieved, relevant):\n",
        "    tp = len(set(retrieved) & set(relevant))\n",
        "    p = tp / len(retrieved) if retrieved else 0\n",
        "    r = tp / len(relevant) if relevant else 0\n",
        "    return p, r\n",
        "\n",
        "def average_precision(ranked_results, relevant_docs):\n",
        "    relevant_set = set(relevant_docs)\n",
        "    precisions = []\n",
        "    num_relevant = 0\n",
        "\n",
        "    for i, doc_id in enumerate(ranked_results, 1):\n",
        "        if doc_id in relevant_set:\n",
        "            num_relevant += 1\n",
        "            precisions.append(num_relevant / i)\n",
        "\n",
        "    return sum(precisions) / len(relevant_set) if relevant_set else 0\n",
        "\n",
        "def ndcg(relevances, k=None):\n",
        "    if k: relevances = relevances[:k]\n",
        "    dcg = sum((2**rel - 1) / np.log2(i + 2) for i, rel in enumerate(relevances))\n",
        "    ideal = sorted(relevances, reverse=True)\n",
        "    idcg = sum((2**rel - 1) / np.log2(i + 2) for i, rel in enumerate(ideal))\n",
        "    return dcg / idcg if idcg > 0 else 0\n",
        "\n",
        "# Example\n",
        "retrieved = [3, 7, 1, 5, 2]\n",
        "relevant = [1, 2, 3, 5]\n",
        "\n",
        "p, r = precision_recall(retrieved, relevant)\n",
        "ap = average_precision(retrieved, relevant)\n",
        "\n",
        "print(f\"Retrieved: {retrieved}\")\n",
        "print(f\"Relevant:  {relevant}\")\n",
        "print(f\"\\nPrecision: {p:.3f}\")\n",
        "print(f\"Recall:    {r:.3f}\")\n",
        "print(f\"F1:        {2*p*r/(p+r):.3f}\")\n",
        "print(f\"AP:        {ap:.3f}\")\n",
        "\n",
        "# nDCG example\n",
        "relevances = [3, 2, 3, 0, 1, 2]  # Graded relevance (0-3)\n",
        "print(f\"\\nnDCG@5:    {ndcg(relevances, k=5):.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0WNGhnmOM3x"
      },
      "source": [
        "## 6. Introduction to Learning to Rank (LTR)\n",
        "\n",
        "### Evolution\n",
        "**Before:** Manual weights\n",
        "```\n",
        "score = 0.5Ã—TF-IDF + 0.3Ã—PageRank + 0.2Ã—URLDepth\n",
        "```\n",
        "\n",
        "**After:** Machine Learning\n",
        "```\n",
        "Learn weights w from training data:\n",
        "score = wâ‚Ã—TF-IDF + wâ‚‚Ã—PageRank + wâ‚ƒÃ—URLDepth + ...\n",
        "```\n",
        "\n",
        "### Training Data Sources\n",
        "- **Explicit:** Human relevance judgments (0-4 scale)\n",
        "- **Implicit:** Clickthrough logs (clicked = more relevant)\n",
        "\n",
        "### Why LTR?\n",
        "âœ… Handles 100+ features automatically  \n",
        "âœ… Learns from user behavior  \n",
        "âœ… Outperforms manual tuning  \n",
        "âœ… Enables personalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-bOvjU3OM3x"
      },
      "source": [
        "## 7. Three Approaches to LTR\n",
        "\n",
        "### Pointwise: Predict Relevance Score\n",
        "- **Input:** Features of (query, doc)\n",
        "- **Output:** Relevance score (e.g., 0-4)\n",
        "- **Method:** Regression or classification\n",
        "- **Example:** Linear regression, neural network\n",
        "\n",
        "### Pairwise: Predict Relative Order\n",
        "- **Input:** Features of (query, docâ‚, docâ‚‚)\n",
        "- **Output:** Which doc should rank higher?\n",
        "- **Method:** Classification on pairs\n",
        "- **Example:** RankSVM, RankNet, LambdaRank\n",
        "\n",
        "### Listwise: Optimize Entire Ranking\n",
        "- **Input:** Features of (query, [docâ‚, docâ‚‚, ..., docâ‚™])\n",
        "- **Output:** Optimal ordering\n",
        "- **Method:** Directly optimize IR metric\n",
        "- **Example:** ListNet, AdaRank"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WVSXSJJhOM3x"
      },
      "outputs": [],
      "source": [
        "# Comparison visualization\n",
        "approaches = ['Pointwise', 'Pairwise', 'Listwise']\n",
        "metrics = {\n",
        "    'Simplicity': [9, 6, 3],\n",
        "    'Performance': [6, 8, 9],\n",
        "    'Training Speed': [9, 6, 3],\n",
        "    'Direct Optimization': [3, 6, 9]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(metrics, index=approaches)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "df.plot(kind='bar', ax=ax)\n",
        "ax.set_ylabel('Score (0-10)')\n",
        "ax.set_title('Comparison of LTR Approaches')\n",
        "ax.set_xticklabels(approaches, rotation=0)\n",
        "ax.legend(title='Aspect')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_OnPhKiOM3x"
      },
      "source": [
        "## 8. Pointwise Approach\n",
        "\n",
        "### Classification: Binary Relevance\n",
        "$$y = f(\\vec{x}) \\in \\{0, 1\\}$$\n",
        "\n",
        "### Regression: Graded Relevance\n",
        "$$y = f(\\vec{x}) \\in [0, 4]$$\n",
        "\n",
        "**Pros:** Simple, uses standard ML algorithms  \n",
        "**Cons:** Doesn't optimize for ranking, treats docs independently"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hyBTMuEeOM3y"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Features: [tfidf, bm25, pagerank, doc_len, query_coverage]\n",
        "X_train = np.array([\n",
        "    [0.9, 0.95, 0.8, 130, 1.0],\n",
        "    [0.8, 0.85, 0.7, 110, 0.9],\n",
        "    [0.5, 0.6, 0.4, 80, 0.5],\n",
        "    [0.3, 0.4, 0.2, 60, 0.3],\n",
        "    [0.1, 0.15, 0.05, 30, 0.1],\n",
        "])\n",
        "y_train = np.array([4, 3, 2, 1, 0])  # Relevance grades\n",
        "\n",
        "# Train pointwise model\n",
        "model = RandomForestRegressor(n_estimators=50, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Test\n",
        "X_test = np.array([\n",
        "    [0.85, 0.88, 0.75, 115, 0.92],\n",
        "    [0.4, 0.5, 0.3, 70, 0.4],\n",
        "    [0.92, 0.96, 0.85, 135, 0.98],\n",
        "])\n",
        "\n",
        "predictions = model.predict(X_test)\n",
        "print(\"Pointwise Predictions (Relevance Scores):\")\n",
        "for i, score in enumerate(predictions, 1):\n",
        "    print(f\"Doc {i}: {score:.2f}\")\n",
        "\n",
        "# Rank by score\n",
        "ranked = np.argsort(predictions)[::-1]\n",
        "print(\"\\nRanking:\", [f\"Doc{i+1}\" for i in ranked])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qv2gUBxQOM3y"
      },
      "source": [
        "## 9. Pairwise Approach: RankSVM\n",
        "\n",
        "### Key Idea\n",
        "Learn from preference pairs: $(d_i, d_j)$ where $d_i \\succ d_j$\n",
        "\n",
        "### Optimization\n",
        "$$\\min_{\\vec{w}} \\frac{1}{2}||\\vec{w}||^2 + C\\sum_{(i,j)} \\xi_{ij}$$\n",
        "\n",
        "Subject to:\n",
        "$$\\vec{w} \\cdot (\\vec{x_i} - \\vec{x_j}) \\geq 1 - \\xi_{ij}$$\n",
        "\n",
        "**Intuition:** Learn weight vector $\\vec{w}$ such that more relevant docs score higher\n",
        "\n",
        "### Training Data from Clicks\n",
        "If user clicks doc at position 5 but not position 2:\n",
        "$$d_5 \\succ d_2 \\text{ (generate training pair)}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YC3lqTsROM3y"
      },
      "outputs": [],
      "source": [
        "# Create pairwise training data\n",
        "def create_pairs(X, y):\n",
        "    \"\"\"Generate preference pairs from pointwise data\"\"\"\n",
        "    pairs_X, pairs_y = [], []\n",
        "    for i in range(len(X)):\n",
        "        for j in range(len(X)):\n",
        "            if y[i] > y[j]:\n",
        "                pairs_X.append(X[i] - X[j])\n",
        "                pairs_y.append(1)\n",
        "    return np.array(pairs_X), np.array(pairs_y)\n",
        "\n",
        "X_pairs, y_pairs = create_pairs(X_train, y_train)\n",
        "print(f\"Generated {len(X_pairs)} preference pairs from {len(X_train)} documents\")\n",
        "print(f\"Example pair (difference vector): {X_pairs[0]}\")\n",
        "\n",
        "# Train SVM on pairs\n",
        "from sklearn.svm import LinearSVC\n",
        "ranksvm = LinearSVC(random_state=42)\n",
        "ranksvm.fit(X_pairs, y_pairs)\n",
        "print(\"\\nâœ… RankSVM trained on pairwise preferences\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HPpqL2a6OM3y"
      },
      "source": [
        "## 10. Pairwise Approach: RankNet\n",
        "\n",
        "### Neural Network Architecture\n",
        "```\n",
        "Input (features) â†’ Hidden Layers â†’ Output (score)\n",
        "```\n",
        "\n",
        "### Pairwise Probability\n",
        "$$P(d_i \\succ d_j) = \\frac{1}{1 + e^{-\\sigma(s_i - s_j)}}$$\n",
        "\n",
        "Where $s_i = f_{NN}(\\vec{x_i})$ is the neural network score\n",
        "\n",
        "### Loss Function (Cross-Entropy)\n",
        "$$L = -\\bar{P}_{ij}\\log P_{ij} - (1-\\bar{P}_{ij})\\log(1 - P_{ij})$$\n",
        "\n",
        "**Training:** Use gradient descent to minimize loss over all pairs\n",
        "\n",
        "**Advantage:** Smooth, differentiable, scalable to large datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Gbd0pKzOM3y"
      },
      "outputs": [],
      "source": [
        "# RankNet probability calculation\n",
        "def ranknet_prob(score_i, score_j, sigma=1.0):\n",
        "    return 1.0 / (1.0 + np.exp(-sigma * (score_i - score_j)))\n",
        "\n",
        "# Example scores\n",
        "scores = [0.9, 0.7, 0.5, 0.2]\n",
        "\n",
        "print(\"RankNet Pairwise Probabilities P(i > j):\\n\")\n",
        "print(\"     \" + \"  \".join([f\"D{i+1}\" for i in range(len(scores))]))\n",
        "for i in range(len(scores)):\n",
        "    row = f\"D{i+1}  \"\n",
        "    for j in range(len(scores)):\n",
        "        if i == j:\n",
        "            row += \" -   \"\n",
        "        else:\n",
        "            prob = ranknet_prob(scores[i], scores[j])\n",
        "            row += f\"{prob:.2f} \"\n",
        "    print(row)\n",
        "\n",
        "print(\"\\nInterpretation: P(D1 > D4) = 0.99 (very confident D1 should rank higher)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owwPi2K_OM3y"
      },
      "source": [
        "## 11. LambdaRank & LambdaMART\n",
        "\n",
        "### Problem with RankNet\n",
        "All pairwise errors weighted equally, but **top-of-ranking errors matter more!**\n",
        "\n",
        "### LambdaRank Solution\n",
        "Scale gradients by change in IR metric:\n",
        "\n",
        "$$\\lambda_{ij} = \\frac{\\partial L}{\\partial s_i} \\cdot |\\Delta nDCG_{ij}|$$\n",
        "\n",
        "Where $\\Delta nDCG_{ij}$ = change in nDCG if we swap docs $i$ and $j$\n",
        "\n",
        "### LambdaMART\n",
        "**Combines:** LambdaRank + Gradient Boosted Trees (MART)\n",
        "\n",
        "**Architecture:**\n",
        "- Ensemble of decision trees\n",
        "- Each tree trained on lambda gradients\n",
        "- Final score = $\\sum_t \\text{tree}_t(\\vec{x})$\n",
        "\n",
        "**Performance:** Winner of Yahoo! LTR Challenge 2010, industry standard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GT5Xa2q1OM3y"
      },
      "outputs": [],
      "source": [
        "# Demonstrate impact of ranking position on nDCG\n",
        "def show_position_importance():\n",
        "    base_relevance = [3, 2, 1, 1, 0, 0, 0]\n",
        "\n",
        "    # Swap highly relevant doc from position 1 to 3\n",
        "    swap_early = base_relevance.copy()\n",
        "    swap_early[0], swap_early[2] = swap_early[2], swap_early[0]\n",
        "\n",
        "    # Swap low relevant doc from position 5 to 7\n",
        "    swap_late = base_relevance.copy()\n",
        "    swap_late[4], swap_late[6] = swap_late[6], swap_late[4]\n",
        "\n",
        "    ndcg_base = ndcg(base_relevance)\n",
        "    ndcg_early = ndcg(swap_early)\n",
        "    ndcg_late = ndcg(swap_late)\n",
        "\n",
        "    print(\"Impact of Ranking Position on nDCG:\\n\")\n",
        "    print(f\"Original ranking:     nDCG = {ndcg_base:.3f}\")\n",
        "    print(f\"Swap positions 1â†”3:   nDCG = {ndcg_early:.3f} (Î” = {ndcg_early-ndcg_base:.3f})\")\n",
        "    print(f\"Swap positions 5â†”7:   nDCG = {ndcg_late:.3f} (Î” = {ndcg_late-ndcg_base:.3f})\")\n",
        "    print(\"\\nðŸ“Œ Top-position errors hurt nDCG more â†’ LambdaRank focuses there!\")\n",
        "\n",
        "show_position_importance()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5f6aOs2OM3z"
      },
      "source": [
        "## 12. Comparison & Best Practices\n",
        "\n",
        "### Algorithm Comparison\n",
        "\n",
        "| Algorithm | Type | Pros | Cons | Use Case |\n",
        "|-----------|------|------|------|----------|\n",
        "| **Linear Regression** | Pointwise | Simple | Doesn't optimize ranking | Baseline |\n",
        "| **RankSVM** | Pairwise | Good with clicks | Many pairs | Clickthrough data |\n",
        "| **RankNet** | Pairwise | Scalable | Doesn't prioritize top | Large-scale |\n",
        "| **LambdaRank** | Pairwise | Direct metric opt. | Complex | High performance |\n",
        "| **LambdaMART** | Pairwise | State-of-art | Training time | Production |\n",
        "\n",
        "### Feature Engineering Tips\n",
        "**Query-Doc Features:** TF-IDF, BM25, cosine similarity  \n",
        "**Doc Features:** PageRank, length, freshness, authority  \n",
        "**Query Features:** Length, type, popularity  \n",
        "**URL Features:** Depth, domain authority, HTTPS  \n",
        "\n",
        "### Public Datasets\n",
        "- **LETOR:** Microsoft Research benchmark (MQ2007, MQ2008)\n",
        "- **MSLR-WEB:** Large-scale Bing data (10K-30K queries)\n",
        "- **Yahoo! C14:** 30K queries, 700+ features\n",
        "- **Yandex:** Personalized search data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5dMZfkvYOM3z"
      },
      "outputs": [],
      "source": [
        "# Summary visualization\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Timeline\n",
        "years = [2002, 2005, 2006, 2010, 2015, 2020]\n",
        "algorithms = ['RankSVM', 'RankNet', 'LambdaRank', 'LambdaMART', 'Neural LTR', 'BERT Rankers']\n",
        "ax1.plot(years, range(len(years)), 'o-', linewidth=2, markersize=10)\n",
        "for i, (year, algo) in enumerate(zip(years, algorithms)):\n",
        "    ax1.text(year, i, f' {algo}', va='center', fontsize=10)\n",
        "ax1.set_xlabel('Year')\n",
        "ax1.set_title('Evolution of Learning to Rank')\n",
        "ax1.set_yticks([])\n",
        "ax1.grid(axis='x', alpha=0.3)\n",
        "\n",
        "# Performance vs Complexity\n",
        "complexity = [3, 5, 7, 8, 6, 9]\n",
        "performance = [6, 7, 8, 9, 8, 9.5]\n",
        "colors = plt.cm.viridis(np.linspace(0, 1, len(algorithms)))\n",
        "\n",
        "for i, (c, p, algo, col) in enumerate(zip(complexity, performance, algorithms, colors)):\n",
        "    ax2.scatter(c, p, s=200, color=col, alpha=0.7)\n",
        "    ax2.annotate(algo, (c, p), fontsize=8, ha='center')\n",
        "\n",
        "ax2.set_xlabel('Complexity')\n",
        "ax2.set_ylabel('Performance')\n",
        "ax2.set_title('Performance vs Complexity Trade-off')\n",
        "ax2.grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hqvs_Xb5OM3z"
      },
      "source": [
        "## 13. Key Takeaways\n",
        "\n",
        "### Classical IR\n",
        "âœ… **TF-IDF:** Balance term frequency with document frequency  \n",
        "âœ… **Cosine Similarity:** Length-normalized vector similarity  \n",
        "âœ… **Rocchio:** Query refinement using relevance feedback  \n",
        "\n",
        "### Evaluation\n",
        "âœ… **Precision/Recall:** Basic quality measures  \n",
        "âœ… **MAP:** Average precision across queries  \n",
        "âœ… **nDCG:** Considers position + graded relevance  \n",
        "\n",
        "### Learning to Rank\n",
        "âœ… **Pointwise:** Predict individual scores (simple but suboptimal)  \n",
        "âœ… **Pairwise:** Learn relative order (industry standard)  \n",
        "âœ… **LambdaMART:** State-of-the-art, used in production  \n",
        "\n",
        "### Modern Trends (2024)\n",
        "ðŸ”¥ **Transformer Models:** BERT, T5 for semantic matching  \n",
        "ðŸ”¥ **Neural Rankers:** End-to-end deep learning  \n",
        "ðŸ”¥ **Personalization:** User-specific ranking functions  \n",
        "ðŸ”¥ **Multi-task Learning:** Joint optimization of multiple objectives  \n",
        "\n",
        "---\n",
        "\n",
        "### References\n",
        "- Manning et al., \"Introduction to Information Retrieval\" (2008)\n",
        "- Liu, \"Learning to Rank for Information Retrieval\" (2011)\n",
        "- Burges et al., \"Learning to Rank using Gradient Descent\" (RankNet, 2005)\n",
        "- Wu et al., \"Adapting Boosting for Information Retrieval\" (LambdaMART, 2010)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}